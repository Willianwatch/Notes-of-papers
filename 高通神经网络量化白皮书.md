# 高通神经网络量化白皮书

- 神经网络量化会将权重和激活值张量以低bit存储。当从32bit存储变成8bit存储时，存储张量的内存开销变为原来的1/4，而矩阵乘法的计算开销变为原来的1/16。

## 硬件层面的背景

- 我们首先来考察下神经网络加速器里的矩阵乘以向量是怎么做的，如下图所示。为了提升推理的效率，这些硬件单元都是并行做计算。神经网络加速器的两个基本组件是处理单元$C_{n,m}$和加法器$A_n$。计算执行的顺序是：
  1. 把bias $\bold{b}_n$加载到加法器中；
  2. 把weight $\bold{W}_{n,m}$和输入的$\bold{x}_{m}$加载进数组，然后在一个时钟周期内在相应的处理单元$C_{n,m}=\bold{W}_{n,m}\bold{x}_{m}$计算它们的乘积。
  3. 结果在加法器中按照下式相加。
$$A_n=\bold{b}_n+\sum_{m}C_{n,m}$$
以上的操作就是一个**MAC**（*Multiply-Accumulate*）。对于大型矩阵向量乘法，这一步会重复很多次。
![alt text](1733354991410.png)
- 一旦所有的循环都完成了，累加器中的值就会被移回内存中，用于下一个神经网络层。
- 神经网络通常使用FP32的权值和激活值来进行训练。如果我们要在FP32中执行推理，处理元素和累加器将必须支持浮点逻辑，并且我们将需要将32位数据从内存传输到处理单元。
- **MAC操作和数据传输消耗了神经网络推理过程中所花费的大部分能量**。因此，通过对这些量使用较低的比特定点或量化表示，可以获得显著的好处。低位定点表示，如INT8，不仅减少了数据传输量，而且还减少了MAC操作的大小和能量消耗。乘法的成本通常与使用的bit位数呈二次关系，并且定点加法比浮点加法更高效。
- 通过量化权重和激活，我们可以写出累加的<span id="公式3">量化版本</span>：

$$\begin{aligned}
\hat{A}_n & =\widehat{\mathbf{b}}_n+\sum_m \widehat{\mathbf{W}}_{n, m} \widehat{\mathbf{x}}_m \\
& =\widehat{\mathbf{b}}_n+\sum_m\left(s_{\mathbf{w}} \mathbf{W}_{n, m}^{\mathrm{int}}\right)\left(s_{\mathbf{x}} \mathbf{x}_m^{\mathrm{int}}\right) \\
& =\widehat{\mathbf{b}}_n+s_{\mathbf{w}} s_{\mathbf{x}} \sum_m \mathbf{W}_{n, m}^{\mathrm{int}} \mathbf{x}_m^{\mathrm{int}}
\end{aligned}$$

- 上式中有意忽略了偏置量化，因为偏置通常存储在更高的位宽（32位）中，其比例因子取决于权重和激活的比例因子。
- 下图显示了当我们引入量化时，神经网络加速器的变化。为了便于讨论，我们使用INT8举例，但是实际上这可以是任何量化形式。为累加器保持一个更高的位宽是很重要的，典型的是32位位宽。否则，由于在计算过程中计算了很多乘积的和，会有溢出的风险。存储在32位累加器中的激活需要先写入内存，然后才能被下一层使用。为了减少数据传输和下一层操作的复杂性，这些激活被**重量化**（requantization）回INT8，如下图所示。
![alt text](1733498778025.png)

## 均匀仿射量化（Uniform affine quantization）
- 均匀仿射量化，也称为非对称量化，由三个量化参数定义：尺度因子s、零点z和位宽b。比例因子和零点用于将浮点值映射到整数网格，网格的大小取决于位宽。比例因子通常用浮点数表示，并指定量化的步长。零点是一个整数，它确保实零被量化而没有误差。这对于确保zero padding或ReLU等通用操作不会引起量化错误很重要。
- 一旦定义了三个量化参数，我们就可以继续进行量化操作了。从一个实值向量$\bold{x}$开始，我们首先将它映射到无符号整数网格{0，……，$2^b$−1}：
$$
\mathbf{x}_{\mathrm{int}}=\operatorname{clamp}\left(\left\lfloor\frac{\mathbf{x}}{s}\right\rceil+z ; 0,2^b-1\right)
$$
其中$\lfloor\cdot\rceil$是四舍五入操作而clamping定义如下：
$$
\operatorname{clamp}(x ; a, c)= \begin{cases}a, & x<a \\ x, & a \leq x \leq c \\ c, & x>c .\end{cases}
$$
- 为了得到近似实值输入x，我们采用了一个反量化步骤：
$$
\mathbf{x} \approx \widehat{\mathbf{x}}=s\left(\mathbf{x}_{\mathrm{int}}-z\right)
$$
- 结合上述两个步骤，我们可以给出量化函数的一般定义，$q(.)$：
$$
\widehat{\mathbf{x}}=q(\mathbf{x} ; s, z, b)=s\left[\operatorname{clamp}\left(\left\lfloor\frac{\mathbf{x}}{s}\right\rceil+z ; 0,2^b-1\right)-z\right]
$$
- 通过反量化步骤，我们还可以定义量化网格范围$(q_{min}, q_{max})$，其中$q_{min}=-sz$和$q_{max}=s(2^b-1-z)$。在此范围之外的任何$\bold{x}$值都将被截断到其限制，从而导致截断误差。如果我们想减少截断误差，我们可以通过增加尺度因子来扩大量化范围。然而，增加比例因子会导致舍入误差的增加，因为舍入误差位于范围$[-\frac{1}{2}s, \frac{1}{2}s]$.
- **原来$q_{min}, q_{max}$给出的是输入浮点数的范围呀，记住！**
## 对称均匀量化
- 对称量化是一般非对称情况的简化版本。对称量化将零点限制为0。这减少了在[公式3](#公式3)中的积累操作中处理零点偏移量的计算开销。但是偏移量的缺失限制了整数域和浮点域之间的映射。因此，选择有符号还是无符号的网格很重要。
- 对于无符号整数：
