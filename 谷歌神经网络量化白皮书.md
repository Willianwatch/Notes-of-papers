# 谷歌神经网络量化白皮书

## 简介

- 迫切需要一些技术来优化模型，以减少模型规模、更快的推理和降低功耗
  - 设计轻量级模型
  - 通过量化、剪枝和压缩技术减少模型的大小
- 降低任何模型复杂性的一个更简单的方法是降低权重和激活的精度要求（**模型量化**）。这种方法有许多优点：
  - 它广泛适用于一系列的模型和用例。人们不需要开发一个新的模型架构来提高速度。在许多情况下，我们可以从现有的浮点模型开始，快速量化，得到一个几乎没有精度损失的不动点量化模型，而不需要重新训练模型。多个硬件平台和库支持具有量化权重和激活功能的快速推断，因此不需要等待新的硬件开发。
  - 更小的模型占用空间：使用8位量化，人们可以将模型尺寸减少4倍，而精度损失可以忽略不计。这可以在不需要任何数据的情况下完成，因为只有权重是量化的。这也导致了模型更新的下载时间的加快。
  - 用于激活的工作内存和缓存减少：中间计算通常存储在缓存中，以便被深度网络的后期层重用，降低存储数据的精度会导致所需的工作内存减少。具有较低的精度权重和激活功能，允许更好的缓存重用。
  - 更快的计算速度：大多数处理器允许更快地处理8位数据。
  - 低功率：移动8位数据比移动32位浮点数据的效率高4倍。在许多深度体系结构中，内存访问可以主导功耗。因此，减少数据移动量可以对功耗产生显著影响。
以上所有因素都可以转化为更快的推理，由于内存访问和计算的精度降低，典型的加速速度为2-3倍。通过为低精度矢量算法优化的处理器和硬件加速器，可以进一步提高速度和功耗。

## 量化器设计

### 均匀仿射量化

- 考虑一个具有范围$(x_{min},x_{max})$的浮点变量，它需要被量化到范围$(0,N_{levels}-1)$，其中$N_{levels}=256$为8-bit精度。我们推导出了两个参数：Scale（∆）和零点(z)，它们将浮点值映射到整数。比例指定量化器的步长，浮点零点映射到零点。零点是一个整数，确保零被量化而没有误差。这对于确保像零填充这样的常见操作不会导致量化错误很重要的。
- 因此，对于单侧分布，范围$(x_{min},x_{max})$被放宽为包括零。例如，具有范围（2.1，3.5）的浮点变量将被放宽为（0，3.5），然后再量化。请注意，在极端单边分布的情况下，这可能会导致精度的损失。
- 一旦定义了尺度和零点，量化就会进行如<span id="公式1">公式1</span>和<span id="公式2">公式2</span>的操作：

$$
\begin{aligned}
x_{i n t} & =\operatorname{round}\left(\frac{x}{\Delta}\right)+z \\
x_Q & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{i n t}\right)
\end{aligned}
$$
其中
$$
\begin{array}{rlr}
\operatorname{clamp}(a, b, x) & =a & x \leq a \\
& =x & a \leq x \leq b \\
& =b & x \geq b
\end{array}
$$
而反量化的操作是<span id="公式3">公式3</span>：
$$
x_{\text {float }}=\left(x_Q-z\right) \Delta
$$

- 虽然均匀仿射量化器允许以8位的精度存储权重和激活，但由于零点，会有额外的成本。考虑一个权重和一个激活之间的二维卷积，如<span id="公式4">公式4</span>和<span id="公式5&6">公式5&6</span>：

$$
\begin{aligned}
y(k, l, n) & =\Delta_w \Delta_x \operatorname{conv}\left(w_Q(k, l, m ; n)-z_w, x_Q(k, l, m)-z_x\right) \\
y(k, l, n) & =\operatorname{conv}\left(w_Q(k, l, m ; n), x_Q(k, l, m)\right)-z_w \sum_{k=0}^{K-1} \sum_{l=0}^{K-1} \sum_{m=0}^{N-1} x_Q(k, l, m) \\
& -z_x \sum_{k=0}^{K-1} \sum_{l=0}^{K-1} \sum_{m=0}^{N-1} w_Q(k, l, m ; n)+z_x z_w
\end{aligned}
$$

- 一个简单的卷积实现，通过在卷积之前执行零点的添加，由于更宽的（16/32位）操作数，导致吞吐量减少了2倍到4倍。使用上面的方程，注意最后一项是一个常数，其他每一项都需要N倍，这比8位点积多3倍。通过注意到在推理时权重是恒定的，并注意到所有相同大小的卷积核的激活的总和是相同的，可以进一步改进这一点。然而，这需要优化卷积内核。

### 均匀对称量化

- 仿射量化器的一个简化版本是对称量化器，它将零点限制为0。使用对称量化器，转换操作简化为，如<span id="公式7">公式7</span>、<span id="公式8">公式8</span>和<span id="公式9">公式9</span>：

$$
\begin{aligned}
x_{\text {int }} & =\operatorname{round}\left(\frac{x}{\Delta}\right) \\
x_Q & =\operatorname{clamp}\left(-N_{\text {levels }} / 2, N_{\text {levels }} / 2-1, x_{\text {int }}\right) \quad \text { if signed } \\
x_Q & =\operatorname{clamp}\left(0, N_{\text {levels }}-1, x_{\text {int }}\right)
\end{aligned} \quad \text { if un-signed }
$$

- 为了更快地实现SIMD，我们进一步限制了权重的范围。在这种情况下，夹紧装置将被修改为<span id="公式10">公式10</span>和<span id="公式11">公式11</span>：

$$
\begin{array}{lr}
x_Q=\operatorname{clamp}\left(-\left(N_{\text {levels }} / 2-1\right), N_{\text {levels }} / 2-1, x_{\text {int }}\right) & \text { if signed } \\
x_Q=\operatorname{clamp}\left(0, N_{\text {levels }}-2, x_{\text {int }}\right) & \text { if un-signed }
\end{array}
$$